import openai
import json
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
client = openai.OpenAI(api_key=api_key)


# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
audio_path = "/Users/kikuyama/shadow_speak_v2/assets/audio/introduction.wav"

# æ–‡å­—èµ·ã“ã—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
with open(audio_path, "rb") as audio_file:
    print("ğŸ“¤ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§æ–‡å­—èµ·ã“ã—ä¸­...")
    response = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        response_format="verbose_json"  # â† word_timestamps ã¯å‰Šé™¤ï¼ˆAPIæœªå¯¾å¿œãªã®ã§ï¼‰
    )

    # JSONãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸åŒ–
    full_data = response.model_dump()

    # ğŸ“„ segments ã ã‘æŠ½å‡ºã—ã¦ Flutter ç”¨ JSON ã«æ•´å½¢
    segments = [
        {
            "start": round(seg["start"], 2),
            "end": round(seg["end"], 2),
            "text": seg["text"].strip()
        }
        for seg in full_data.get("segments", [])
    ]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open("subtitle_segments.json", "w") as out_file:
        json.dump(segments, out_file, indent=2)

    print("âœ… å­—å¹•ãƒ•ã‚¡ã‚¤ãƒ« subtitle_segments.json ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")